{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import pathlib\n",
    "import json\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load and label samples</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4459\n",
      "Test set size: 1115\n"
     ]
    }
   ],
   "source": [
    "corpus_dir = \"./data\"\n",
    "\n",
    "def loadAndLabelData(corpus_dir):\n",
    "    corpus_dir = pathlib.Path(corpus_dir)\n",
    "    \n",
    "    samples = []\n",
    "    labels = []\n",
    "    \n",
    "    for file_path in corpus_dir.rglob(\"*\"):\n",
    "        f = open(file_path, 'r')\n",
    "        for line in f:\n",
    "            if line[0:4] == 'spam':\n",
    "                labels.append(1)\n",
    "                samples.append(line[4:])\n",
    "            elif line[0:3] == 'ham':\n",
    "                labels.append(0)\n",
    "                samples.append(line[3:])\n",
    "            else:\n",
    "                print('improperly labeled sample')\n",
    "                \n",
    "    return samples, labels\n",
    "\n",
    "samples, labels = loadAndLabelData(corpus_dir)\n",
    "\n",
    "# shuffle samples and labels pairs\n",
    "samples, labels = sklearn.utils.shuffle(samples, labels) \n",
    "\n",
    "# split training and testing sets\n",
    "count = len(samples)\n",
    "test_size = round(count * 0.2)\n",
    "      \n",
    "test_samples = samples[count - test_size : count]\n",
    "samples = samples[0 : count - test_size]\n",
    "      \n",
    "test_labels = labels[count - test_size: count]\n",
    "labels = labels[0 : count - test_size]\n",
    "\n",
    "print('Training set size: ' + str(count - test_size))\n",
    "print('Test set size: ' + str(test_size)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex pattern to remove non-alphanumeric\n",
    "pattern = re.compile('([^\\s\\w]|_)+')\n",
    "\n",
    "def cleanText(text):\n",
    "    # remove non-alphanumeric\n",
    "    text = pattern.sub(' ', text)\n",
    "    \n",
    "    #remove newline and tab\n",
    "    text = text.replace('\\n',' ')\n",
    "    \n",
    "    #remove capitalization\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def getTokens(text):\n",
    "    return re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "\n",
    "samples_tokens = []\n",
    "for sample in samples:\n",
    "    samples_tokens.append(getTokens(cleanText(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Get Document Frequency </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7822 unique tokens\n"
     ]
    }
   ],
   "source": [
    "def updateDF(tokens, df):\n",
    "    tokens = set(tokens) # remove duplicate tokens\n",
    "        \n",
    "    for token in tokens:\n",
    "        if token in df.keys():\n",
    "            df[token] += 1\n",
    "        else:\n",
    "            df[token] = 1\n",
    "            \n",
    "df = dict()\n",
    "\n",
    "for tokens in samples_tokens:\n",
    "    updateDF(tokens, df)\n",
    "    \n",
    "vocab = df.keys()\n",
    "print(str(len(vocab)) + ' unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reduce vocab</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced to 1594 unique tokens\n"
     ]
    }
   ],
   "source": [
    "min_df = 0.001 # remove terms that appear in less than 0.1% of documents\n",
    "max_df = 1#0.99\n",
    "\n",
    "for key in list(df):\n",
    "    if not min_df < df[key] / len(samples) < max_df:\n",
    "        del df[key]\n",
    "        \n",
    "print('reduced to ' + str(len(vocab)) + ' unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Get TF-IDF vectors </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(tokens, df, n_docs):\n",
    "    tfidf = dict.fromkeys(df.keys(), 0)\n",
    "        \n",
    "    for token in tokens:\n",
    "        if token in df.keys():\n",
    "            tfidf[token] += 1 # term occurences\n",
    "            \n",
    "    for token in df.keys():\n",
    "        tfidf[token] /= len(tokens) + 1 # term frequency\n",
    "        idf = math.log((n_docs + 1) / (df[token] + 1))\n",
    "            \n",
    "        tfidf[token] = tfidf[token] * idf\n",
    "        \n",
    "    return np.array(list(tfidf.values()))\n",
    "\n",
    "tfidfs = np.zeros((len(samples), len(df)))\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    sample = samples[i]\n",
    "    tfidfs[i] = getTFIDF(samples_tokens[i], df, len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''max_cos = 0\n",
    "similar = 0\n",
    "\n",
    "vec = getTFIDF(getTokens(cleanText(test_samples[110])), df, len(samples))\n",
    "\n",
    "for i in range(len(tfidfs)):\n",
    "    cos = cosine(vec, tfidfs[i])\n",
    "    if cos > max_cos:\n",
    "        max_cos = cos\n",
    "        similar = i\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Perform KNN classification </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(v1, v2):\n",
    "    mag = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    if mag > 0:\n",
    "        return np.dot(v1, v2) / mag\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def knn(k, xs, ys, x):\n",
    "    cosines = []\n",
    "    \n",
    "    for i in range(len(xs)):\n",
    "        cosines.append(cosine(xs[i], x))\n",
    "        \n",
    "    # indexes of k nearest\n",
    "    nearest = list(range(0, k))\n",
    "    \n",
    "    for i in range(k, len(cosines)):\n",
    "        \n",
    "        # find least similar from nearest list\n",
    "        least_sim_index = 0\n",
    "        min_cos = cosines[nearest[least_sim_index]]\n",
    "        \n",
    "        for j in range(1, k):\n",
    "            cos = cosines[nearest[j]]\n",
    "            if cos < min_cos:\n",
    "                min_cos = cos\n",
    "                least_sim_index = j\n",
    "                \n",
    "        if min_cos < cosines[i]:\n",
    "            nearest[least_sim_index] = i\n",
    "            \n",
    "    n = 0\n",
    "    for i in nearest:\n",
    "        n += ys[i]\n",
    "        \n",
    "    if n > k / 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# test accuracy\n",
    "    \n",
    "correct = 0\n",
    "\n",
    "for i in range(len(test_samples)):\n",
    "    #tfidf = 0.5 * getTFIDF(test_documents[i]['title']) + 0.5 * getTFIDF(test_documents[i]['text'])\n",
    "    \n",
    "    test_sample_tfidf = getTFIDF(getTokens(test_samples[i]), df, len(samples))\n",
    "    \n",
    "    if knn(5, tfidfs, labels, test_sample_tfidf) == test_labels[i]:\n",
    "        correct += 1\n",
    "        \n",
    "print(correct / len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8573991031390135\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "\n",
    "for label in test_labels:\n",
    "    if label == 1:\n",
    "        n = n + 1\n",
    "        \n",
    "print(1 - n / len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn(1, tfidfs, labels, tfidfs[156])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cos = 0\n",
    "similar = 0\n",
    "\n",
    "vec = getTFIDF(getTokens(cleanText(test_samples[110])), df, len(samples))\n",
    "\n",
    "for i in range(len(tfidfs)):\n",
    "    cos = cosine(tfidfs[500], tfidfs[i])\n",
    "    if cos > max_cos:\n",
    "        max_cos = cos\n",
    "        similar = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine",
   "language": "python",
   "name": "mine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
